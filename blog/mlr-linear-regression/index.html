<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title>NBA Machine Learning #2 - Linear Regression with mlr package - Per 48 minutes</title>
    <link rel="stylesheet" href="/css/crab.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=0.5, maximum-scale=3.0">
		
    <meta name="generator" content="Hugo 0.35" />

	

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116740634-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116740634-1');
</script>

    

  </head>
  <body>

    <div id="container">

      <div id="header">

        <div id="site-logo">
          
            <img src="/./img/shots-logo.jpg">
          
        </div>

	<div id="site-title"><a href="/">Per 48 minutes</a></div>
	<div id="site-slogan">NBA data analysis. Statistics. Machine Learning.</div>

      </div>

            <nav>
	<ul class="first">
          
            <li><a href="/blog/">Blog</a>
	    
	    </li>
	  
            <li><a href="/about/">About</a>
	    
	    </li>
	  
	</ul>
      </nav>


      <div id="content">

        <div id="article">
	  <h1>NBA Machine Learning #2 - Linear Regression with mlr package</h1>


<p class="timestamp">May 15, 2018</p>

<div id="TOC">
<ul>
<li><a href="#about-series">About series</a></li>
<li><a href="#linear-regression">Linear Regression</a></li>
<li><a href="#methods-of-achieving-the-best-fit">Methods of achieving the best fit</a><ul>
<li><a href="#ordinary-least-squares---ols">Ordinary Least Squares - OLS</a></li>
<li><a href="#one-predictor">One predictor</a></li>
<li><a href="#more-predictors-matrix-based-formula">More predictors (matrix-based formula)</a></li>
<li><a href="#stochastic-gradient-descent-method">Stochastic Gradient Descent method</a></li>
</ul></li>
<li><a href="#assumptions">Assumptions</a><ul>
<li><a href="#linear-relationship">Linear relationship</a></li>
<li><a href="#correlation">Correlation</a></li>
<li><a href="#homoscedasticity-of-error-terms">Homoscedasticity of error terms</a></li>
<li><a href="#normal-distribution-of-error-terms">Normal distribution of error terms</a></li>
<li><a href="#autocorrelation-of-error-terms">Autocorrelation of error terms</a></li>
<li><a href="#no-multicollinearity">No Multicollinearity</a></li>
<li><a href="#outliers">Outliers</a></li>
<li><a href="#high-leverage-observations">High leverage observations</a></li>
<li><a href="#scaling-variables">Scaling variables</a></li>
</ul></li>
<li><a href="#building-a-model---example-linear-regression-with-mlr-package">Building a model - example Linear Regression with mlr package</a><ul>
<li><a href="#correlation-based-feature-selection">Correlation based feature selection</a></li>
<li><a href="#first-basic-model">First basic model</a></li>
<li><a href="#model-with-excludedscaled-observations">Model with excluded/scaled observations</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<div id="about-series" class="section level2">
<h2>About series</h2>
<p>So I came up with an idea to write series of articles describing how to apply machine learning algorithms on some examples of NBA data. Got to say that plan is quite ambitious - I want to start with machine learning basics, go through some common data problems, required preprocessing tasks and obviously, algorithms. Starting with Linear Regression and hopefully finishing one day with Neural Networks or any other mysterious blackbox. Articles are going to cover both theory and practice (code) and data will be centered around NBA.</p>
<p>Most of the tasks will be done with mlr package since I try to master it and I am looking for motivation. Datasets and scripts will be uploaded on my github, so feel free to use them for own practice!</p>
<hr />
<ul>
<li><a href="/blog/mlr-model-building/">1. Process of building a model</a></li>
<li><a href="/blog/mlr-linear-regression/">2. Linear Regression with mlr package</a></li>
<li><a href="/blog/mlr-splines-loess-gams/">3. Splines, LOESS and GAMs with mlr package</a></li>
</ul>
<hr />
<pre class="r"><code>library(tidyverse)
library(broom)
library(mlr)
library(GGally)
library(visdat)

options(scipen = 999)

nba &lt;- read_csv(&quot;../../data/mlmodels/nba_linear_regression.csv&quot;) %&gt;% glimpse</code></pre>
<pre><code>## Observations: 78,739
## Variables: 13
## $ PTS         &lt;int&gt; 23, 30, 22, 41, 20, 8, 33, 33, 28, 9, 15, 24, 21, ...
## $ FG2A        &lt;int&gt; 10, 17, 12, 14, 8, 7, 24, 12, 13, 11, 12, 13, 13, ...
## $ FG3A        &lt;int&gt; 3, 3, 3, 6, 1, 2, 1, 4, 0, 2, 1, 0, 0, 2, 1, 3, 1,...
## $ FTA         &lt;int&gt; 4, 9, 2, 21, 14, 4, 10, 15, 7, 4, 6, 5, 6, 12, 13,...
## $ AST         &lt;int&gt; 5, 6, 1, 6, 10, 6, 3, 3, 6, 8, 4, 8, 4, 5, 7, 5, 7...
## $ OREB        &lt;int&gt; 0, 2, 2, 1, 2, 0, 2, 4, 1, 0, 2, 0, 2, 1, 6, 0, 0,...
## $ PF          &lt;int&gt; 5, 4, 1, 3, 5, 2, 2, 4, 3, 3, 3, 4, 2, 1, 2, 3, 5,...
## $ TOV         &lt;int&gt; 5, 4, 4, 5, 2, 1, 0, 6, 4, 3, 6, 1, 1, 0, 1, 6, 2,...
## $ STL         &lt;int&gt; 2, 0, 2, 2, 1, 1, 4, 4, 1, 0, 0, 2, 0, 2, 4, 5, 0,...
## $ PACE        &lt;dbl&gt; 104.44, 110.01, 94.88, 103.19, 92.42, 93.35, 106.2...
## $ MINS        &lt;dbl&gt; 35.0, 33.9, 36.1, 41.6, 34.4, 36.7, 39.9, 34.5, 35...
## $ PLAYER_NAME &lt;chr&gt; &quot;Giannis Antetokounmpo&quot;, &quot;Giannis Antetokounmpo&quot;, ...
## $ GAME_DATE   &lt;date&gt; 2017-02-03, 2017-02-04, 2017-02-08, 2017-02-10, 2...</code></pre>
<pre class="r"><code>PTS &lt;- nba %&gt;% pull(PTS)
FG2A &lt;- nba %&gt;% pull(FG2A)
FTA &lt;- nba %&gt;% pull(FTA)</code></pre>
<hr />
</div>
<div id="linear-regression" class="section level2">
<h2>Linear Regression</h2>
<p>Linear regression is an algorithm enabling us to find linear relationship between continous variables. Simple regression consists of one predictor and obviously, one response variable. Multiple regression has more predictors. It’s important to remember that equations in linear regression are not deterministic but they are statistical - which means they always have some error term added to response and therefore they are not perfect.</p>
<p>But nothing really is… basketball? Maybe basketball.</p>
<p>“All models are wrong but some are useful” …bla..bla..bla</p>
<p>It’s one of the most common and one of the oldest supervised machine learning - before it was called machine learning - algorithms, used for prediction.</p>
<p><br></p>
<p>It’s achieved by calculating the best fit of predictor variables to outcome variable. Best fit function consists of predictors <strong>x1, x2… xi</strong>, their coefficients <strong>b1, b2… bi</strong> and intercept value <strong>b0</strong>.</p>
<p><br></p>
<p><span class="math display">\[ \hat{y} =  b_0 + b_1x_1 + b_2x_2 + b_ix_i \]</span>
<br>
Whereas real value is defined by formula:</p>
<p><span class="math display">\[ y = b_0 + b_1x_1 + b_2x_2  + b_ix_i + \epsilon_n\]</span>
<br></p>
<p>Where ϵ (epsilon) stands for residual that could not be defined by linear coefficients. It’s error term*. Random value changing slightly our outcome values. It’s small granule added to perfect linear world.</p>
<ul>
<li><p>It’s when you always spend 60% of your income, but this month you decided to buy 2 more chocolates.</p></li>
<li><p>It’s when you always make 40% of your 3 point attempts, but this game ball rolled out of the rim on the last shot.</p></li>
<li><p>It’s that small human error added to always right mathematic equation.</p></li>
</ul>
<p>It represents nonlinearity, unpredictable situations, measurements errors and ommitted variables</p>
<pre><code>*Residuals and error terms are not exactly the same thing &lt;roaring statistician in background&gt;.
As also coefficients of the model are not real life coefficients. 
In reality there are at least two lines:

- One line is real regression line true for whole population, which formula is unknown 
(This has error terms and real coefficients)

- Second line is our linear regression model line which we calculated using sample of population 
(Here we got residuals and estimations of coefficients).

So residuals are estimations of real life error terms

It&#39;s good to remember the difference, but in my mind, in machine learning it&#39;s not super important 
to differ between those two.

If there is real-life best fit line that is unknown... should we care about it?
Is it even real? 
What&#39;s real?

We only know the line we got estimated from the sample. We also know that somewhere there is different
line based on whole population</code></pre>
<hr />
</div>
<div id="methods-of-achieving-the-best-fit" class="section level2">
<h2>Methods of achieving the best fit</h2>
<p>Here are listed most common estimation methods with explanation, variations and code to showcase and help to understand the idea. I tend to avoid complex math formulas - first of all, I don’t really think that dull gaping at math formulas can be any good approach for effective learning and UNDERSTANDING the method. Secondly, it’s a blog, not a book. Its purpose is to explain clearly the thing that authors of books failed to clarify. Mic dropped.</p>
<p><br></p>
<div id="ordinary-least-squares---ols" class="section level3">
<h3>Ordinary Least Squares - OLS</h3>
<hr />
</div>
<div id="one-predictor" class="section level3">
<h3>One predictor</h3>
<p>The basic and most common estimation method for linear regression. It’s based on minimizing the sum of distances between actual response observations and set of predictors. In other words, minimazing residual sum of squares <strong>RSS</strong>.</p>
<p><br></p>
<p>RSS - residual sum of squares</p>
<p><span class="math display">\[  RSS = \sum{(y_i - \hat{y_i})^2}  \]</span></p>
<p>If we use formula for model prediction:</p>
<p><span class="math display">\[ \hat{y} =  b_0 + b_nx_n \]</span></p>
<p><span class="math display">\[  RSS = \sum_{i = 1}^{n}{(y_i -  (b_0 + b_nx_n)_i)^2}  \]</span></p>
<p>Our goal is to minimize RSS, knowing y and x values - that means that we have to find b0 and bn parameters that will minimize RSS. We do that by using derivatives (RSS is a function of b0 and bn, minimum is when the slope of function is equal to 0, derivative finds slope at given point).</p>
<p>We are reaching the scary point here!</p>
<ul>
<li>Partial derivative of RSS by b0 (intercept):</li>
</ul>
<p><span class="math display">\[  -2\sum_{i=1}^{n}{(y_i - (b_0 + b_1x_i) )}   \]</span></p>
<ul>
<li>Partial derivative of RSS by b1 (x1 coefficient)</li>
</ul>
<p><span class="math display">\[ -2\sum_{i=1}^{n}{(y_i - (b_0 + b_1x_1i) )x_1i} \]</span></p>
<ul>
<li>We need to find minimum of a function - this is the place when derivative is equal to 0</li>
</ul>
<p><span class="math display">\[ -2\sum_{i=1}^{n}{(y_i - (b_0 + b_1x_i) )} = 0 \]</span></p>
<p><span class="math display">\[ -2\sum_{i=1}^{n}{(y_i - (b_0 + b_1x_i) )x_i} = 0 \]</span></p>
<ul>
<li>Which gives us pair of equations to calculate our coefficients b0 and b1. After some steps of math calculations we should get:</li>
</ul>
<p><span class="math display">\[ b_1 = - \frac{n\bar{x}\bar{y} - \sum{yx}}{\sum{x^2 - n\bar{x^2}}}\]</span></p>
<p>…or just…</p>
<p><span class="math display">\[  b_1 = \frac{COV(x,y)}{VAR(x)}  \]</span></p>
<p>…and…</p>
<p><span class="math display">\[ b_0 = \bar{y} - b_1\bar{x}\]</span></p>
<p>…As a last piece of simple linear regression formula.</p>
<p>Where n is number of observations, x is predictor variable, y is response variable.</p>
<p><br></p>
<p>So, in easier to read, coding example:</p>
<ul>
<li>response: PTS</li>
<li>predictor: FG2A</li>
</ul>
<pre class="r"><code>n = length(PTS)

b1 &lt;- - (mean(PTS)*mean(FG2A)*n - sum(PTS*FG2A))/(sum(FG2A^2) - n*mean(FG2A)^2)
b0 &lt;- mean(PTS) - b1*mean(FG2A)

cat(paste(&#39;b0 = &#39;,round(b0,4),&#39;b1 =&#39;,round(b1,4)))</code></pre>
<pre><code>## b0 =  2.1704 b1 = 1.3956</code></pre>
<p>or</p>
<pre class="r"><code>cov(FG2A,PTS)/var(FG2A)</code></pre>
<pre><code>## [1] 1.395591</code></pre>
<p>We get slope and coefficient of simple linear regression. Let’s compare it with built-in R function:</p>
<pre class="r"><code>lm(PTS ~ FG2A)</code></pre>
<pre><code>## 
## Call:
## lm(formula = PTS ~ FG2A)
## 
## Coefficients:
## (Intercept)         FG2A  
##       2.170        1.396</code></pre>
<p>Coefficients match and that is very good news!</p>
<hr />
</div>
<div id="more-predictors-matrix-based-formula" class="section level3">
<h3>More predictors (matrix-based formula)</h3>
<p>Now it’s time to present the solution for multiple linear regression. When we have more than one predictor it is MUCH easier to use matrix-based formula.</p>
<p><span class="math display">\[ \beta = (X^tX)^{-1}X^tY \]</span>
Where Beta stands for coefficients, X for matrix of predictors, Y for vector of response variable. Key thing! Add 1 as intercept to the X matrix if you try to calculate OLS manually, otherwise coefficients will not match the LM() function, which is quite annoying.</p>
<p>(Yes. It took me some time to realize it)</p>
<pre class="r"><code>x &lt;- nba %&gt;%
  select(FG2A, FTA) %&gt;%
  mutate(intercept = 1) %&gt;%  
  as.matrix()

y &lt;- nba %&gt;% pull(PTS)

solve(t(x)%*%x)%*%t(x)%*%y</code></pre>
<pre><code>##                [,1]
## FG2A      1.0868669
## FTA       0.9940133
## intercept 1.7486047</code></pre>
<p>Also, we have to remember that R uses QR decomposition for matrices, to make the numerical calculation more stable. So the most often used formula is:</p>
<p><span class="math display">\[  \beta = R^{-1}Q^tY\]</span></p>
<pre class="r"><code>qrm &lt;- qr(x)
solve(qr.R(qrm))%*%t(qr.Q(qrm))%*%y</code></pre>
<pre><code>##                [,1]
## FG2A      1.0868669
## FTA       0.9940133
## intercept 1.7486047</code></pre>
<p>Let’s check with R function again!</p>
<pre class="r"><code>lm(PTS ~ FG2A + FTA)</code></pre>
<pre><code>## 
## Call:
## lm(formula = PTS ~ FG2A + FTA)
## 
## Coefficients:
## (Intercept)         FG2A          FTA  
##       1.749        1.087        0.994</code></pre>
<p>Coefficients seem to match again, which makes me really happy.</p>
<hr />
<p><br></p>
</div>
<div id="stochastic-gradient-descent-method" class="section level3">
<h3>Stochastic Gradient Descent method</h3>
<p>This is second estimation method I would like to present. Still not as common as OLS for linear regression models. But it’s very important to know it, especially because it is used in many others algorithms (neural networks for example). It’s advantegous when we have very large amount of data or when it is very sparse. Stochastic Gradient descent is also quite effective when we have multicollinearity in out data.</p>
<p>Gradient descent is algorithm minimizing functions. Given starting set of parameters, algorithm iteratively moves towards the best set of parameters (resulting in function minimum). On each step algorithm calculates gradient on one observation, and then updates coefficients. There are variations of that approach, mostly using batches of observations and/or iterations of process above.</p>
<p>Given simple linear regression, we are looking to minimize error of algorithm by optimizing slope and intercept parameters.</p>
<p><br></p>
<p><span class="math display">\[  MSE = \frac{1}{n}\sum_{i = 1}^{n}{(y_i -  (b_0 + b_nx_n)_i)^2}  \]</span>
Where MSE is mean squared error, b0 is intercept, bn is a slope.</p>
<p>Once again, we are looking for function gradients and we will find them using partial derivatives:</p>
<p><span class="math display">\[ \theta = \frac{-2}{n}\sum_{i=1}^{n}{(y_i - (b_0 + b_nx_i) )}\]</span></p>
<p><span class="math display">\[ \theta = \frac{-2}{n}\sum_{i=1}^{n}{(y_i - (b_0 + b_nx_i) )x_i}\]</span>
Every iteration will update the coefficients by the gradient value multiplied by learning rate. It will be repeated till algorithm converges. Each step consists of one data point or set of them, dependent on the specific version of algorithm. Learning rate is there to control the pace of modifications. We don’t want to make too big changes at each step, because we can miss the local minimum.</p>
<p><span class="math display">\[ \theta_{new} = \theta_{start}  - L_{earning}R_{ate} * \theta_{gradient} \]</span></p>
<p>Code below shows implementation of Stochastic Gradient Descent. Starting points are intercept and slope both equal to -1. Learning rate is set to 0.04, why not. Gradient is calculated in a loop (soRRy) for each one of i observations. Animation will show prediction for every 5000 step.</p>
<pre class="r"><code>library(gganimate)
library(ggplot2)

n = length(PTS)
intercept = -1
slope = -1
LR = 0.04

prediction &lt;- intercept + slope*FG2A
ds_show &lt;- data.frame(prediction, FG2A, PTS, intercept, slope, step = 0, stringsAsFactors = F)

i &lt;- 1
for(i in 1:n){
  
  slope &lt;- slope - LR*(-2/n)*sum(PTS[i]*FG2A[i] - intercept*FG2A[i] - slope*FG2A[i]^2)
  intercept &lt;- intercept - LR*(-2/n)*sum(PTS[i] - intercept - slope*FG2A[i])
  

  if(i %% 5000 == 0){
      prediction &lt;- intercept + slope*FG2A
      ds_stage &lt;-  data.frame(prediction, FG2A, PTS, intercept, slope, step = i, stringsAsFactors = F)
      ds_show &lt;- bind_rows(ds_show,ds_stage)
  }

  
}

animated &lt;- ggplot(data = ds_show, aes(frame = step)) +
  geom_line(aes(x = FG2A,y = prediction), color = &#39;red&#39;)  + 
  geom_point(aes(x = FG2A,y = PTS), color = &#39;blue&#39;)  + 
  theme_bw()

gganimate(animated)</code></pre>
<div class="figure">
<img src="../sgd.gif" title="Stochastic Gradient Descent" alt="Stochastic Gradient Descent presentation" />
<p class="caption">Stochastic Gradient Descent presentation</p>
</div>
<hr />
</div>
</div>
<div id="assumptions" class="section level2">
<h2>Assumptions</h2>
<div id="linear-relationship" class="section level3">
<h3>Linear relationship</h3>
<p>We assume that there is linear and additive relationship between explonatory data and outcome variable. If we apply linear model to non-linear problem in real world, we get high-biased model.</p>
<pre class="r"><code>ggplot()+
  geom_point(aes(x = PTS, y = FG2A)) + theme_bw()</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Scatter rlot above presents linear relationship between 2 variables. It indicates that we can use linear regression to predict one from another. It’s also good to calculate correlation.</p>
<pre class="r"><code> cor(FG2A, PTS)</code></pre>
<pre><code>## [1] 0.7718064</code></pre>
</div>
<div id="correlation" class="section level3">
<h3>Correlation</h3>
<p>Correlation is statistical measure of how strong is linear relationship between 2 variables. It doesn’t imply causation, as every statistics person likes to say. It’s quite useful metric for finding right variables for linear regression, when we want to have predictors highly correlated with repsonse variable, but not correlated with each other (to avoid multicollinearity).</p>
<p>Pearson’s correlation formula:</p>
<p><span class="math display">\[ cor(x,y) = \frac{cov(x,y)}{sd(x)sd(y)} \]</span></p>
<p>where cov is covariance and sd stands for standard deviation.</p>
<p>Pearson’s correlation can take value from range &lt;-1;1&gt; where 0 stands for no correlation, -1 for perfect negative correlation and 1 for perfect positive correlation.</p>
<p><br></p>
</div>
<div id="homoscedasticity-of-error-terms" class="section level3">
<h3>Homoscedasticity of error terms</h3>
<p>This always makes me ask a question:</p>
<p>Was it really necessary?</p>
<p>Was coming up with word that has 17 letters that much more efficient than calling it Constant variance?</p>
<p>Why can’t we just call it constant variance and enjoy simpler life when we use words that we understand, not trying to look snobbish?</p>
<p>Anyway, it’s called homoscedasticity and it means constant variance across observations. If variance changes with observations, then we get heteroscedasticity and it’s not good for our model. We can see it on residuals plot:</p>
<pre class="r"><code>ds &lt;- nba %&gt;%
  select(PTS,FG2A, FG3A, FTA) %&gt;%
  arrange(FG2A) %&gt;%
  as.data.frame()

slr &lt;- lm(PTS ~ FG2A, ds)
plot(slr, which = 3)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Plot above shows that unfortunately our simple regression model has heteroscedkadt…. it doesn’t have constant variance with increasing X. Error terms are getting bigger with increasing predictor. That means that our model is ok when it predicts on lower values, but is getting worse when independent variable increases.</p>
<p>I can tell you that it means that our estimator is no longer blue, or OLS assumption is wrong…but the most important information for us is now that fact that our model is not fitted perfectly:</p>
<ul>
<li>Model is worse than goodness of fit measures indicate</li>
<li>Predictions on larger values have larger errors, so we can no longer be highly confident in predictions</li>
<li>We need to use transformed variable (log-scaled, scaled, etc.) instead of basic one.</li>
</ul>
<p>Important to note, that everytime we witness something different from complete randomness in residuals, then we know that our predictors are missing something.</p>
<hr />
<p>Statistical tests for homoscedaskicity:</p>
<p><strong>Breush-Pagan test</strong></p>
<p>Idea behind this test is to run regression model which residuals of base model are standing for response on independent predictors.</p>
<p>null hypothesis: homoscedaskicity
alternative: heteroscedaskicity</p>
<pre class="r"><code>lmtest::bptest(slr)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  slr
## BP = 2952.1, df = 1, p-value &lt; 0.00000000000000022</code></pre>
<p>P-value is super small, so we reject null hypothesis, which confirms non-constant variance</p>
<hr />
<p>Dealing with heteroscedaskicity in the model:</p>
<p>We need to either transform variables or use weighted OLS to estimate model’s coefficients.</p>
<p><strong>Weighted OLS</strong></p>
<p>It means weighing observations to reduce impact of larger values, for example doing:</p>
<pre class="r"><code>slr_w = lm(PTS ~ FG2A, data = ds, weights = 1/(FG2A))</code></pre>
<p><strong>Log Scaling predictor</strong></p>
<pre class="r"><code>slr_log = lm(PTS ~ log(FG2A), data = ds)</code></pre>
<p><br></p>
</div>
<div id="normal-distribution-of-error-terms" class="section level3">
<h3>Normal distribution of error terms</h3>
<p>We can check if residauls are normally distributed by looking at residuals vs fitted values plot. Ideally scatter points would have common shape across all fitted variables, meaning that the numbers are random. Below you can see that it is not a case. There is significant trend in how residuals values are decreasing when fitted values increase.</p>
<pre class="r"><code>plot(slr$fitted.values,slr$residuals)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>We can also use quantile-quantile (real vs theoretical quantiles) plot of residuals:</p>
<pre class="r"><code>plot(slr, which = 2)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Which shows even better that residuals are not normally distributed and that is big problem for our model.</p>
<hr />
<p>Statistical tests for normal distribution:</p>
<p><strong>Shapiro-Wilk test</strong></p>
<p>This test is biased by sample size i.e. the larger the sample the higher chance to get statistically significant result (always look on q-q plot)</p>
<p>null hypothesis: residuals normally distributed
alternative: residuals are not normally distributed</p>
<pre class="r"><code>shapiro.test(slr$residuals[1:5000])</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  slr$residuals[1:5000]
## W = 0.5889, p-value &lt; 0.00000000000000022</code></pre>
<p>In example above we reject the null hypothesis and confirm non-normality that visualizations presented above.</p>
<p>There are more statistical tests for normality added in nortest package, its worth checking and using more than one test for comparisons.</p>
<p><br></p>
</div>
<div id="autocorrelation-of-error-terms" class="section level3">
<h3>Autocorrelation of error terms</h3>
<p>Autocorellation happens when residuals are dependent on previous residuals values (there is regression between current residual and its lagged values).</p>
<p>If a model is affected by autocorellation, than we know that are predictors are missing some important signal that was included in residuals. We probably need to add some new predictor.</p>
<p>Residual plot with autocorellation presents obvious trend going on.</p>
<hr />
<p>Statistical tests for autocorrelation:</p>
<p><strong>Durbin-Watson test</strong></p>
<p>DW statistic checks what part of all residuals is covered by lagged values.</p>
<p>Durbin-Watson statistic lies in range between 0 and 4 with critical values Dl AND Du</p>
<ul>
<li>0:Dl -&gt; positive autocorrelation</li>
<li>Dl:Du -&gt; no autocorrelation</li>
<li>Du:4 -&gt; negative autocorrelation</li>
</ul>
<pre class="r"><code>lmtest::dwtest(slr)</code></pre>
<pre><code>## 
##  Durbin-Watson test
## 
## data:  slr
## DW = 1.4629, p-value &lt; 0.00000000000000022
## alternative hypothesis: true autocorrelation is greater than 0</code></pre>
<p>So to all of this simple linear regression model’s problems I need to add autocorrelation as well.</p>
<p>While building linear regression we need to always remember about residual graphs. Non-normal distribution, heteroscedaskicity and autocorrelation are things we need to account for and avoid while building the model. They make predictions less confident, error terms are growing or are falsely low, we might lacking some important information in the data.Always check residual graphs.</p>
<p><br></p>
</div>
<div id="no-multicollinearity" class="section level3">
<h3>No Multicollinearity</h3>
<p>If predictors are correlated with each other then it’s hard to tell which one really is significant in predicting response values. It can also add negative noise to coefficients during model training, so it is always good to check for correlation between independent variables.</p>
<p>First what we can do is to use correlation matrix <strong>before</strong> model training, and then pick variables that are highly correlated with response and not correlated with any other predictors.</p>
<pre class="r"><code>cm &lt;- cor(nba[,1:8])
corrplot::corrplot(cm, method = &#39;number&#39;, diag = F, type = &#39;upper&#39;) #plot matrix</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Obviously when we have lots of variables the only thing we can do is program the selection or use</p>
<hr />
<p><strong>Variance Inflation Factors</strong></p>
<p>VIF is the ratio of variance in a model with multiple predictors, divided by the variance of a model with only one predictor. It presents magnitude of multicollinearity of each variable.</p>
<p>It is assumed that if value is larger than 5 then there is a problem and that variable should be removed or combined with correlated one.</p>
<pre class="r"><code>ds_vif &lt;- nba %&gt;%
  select(PTS,FG2A, FG3A, FTA, AST, OREB, PF, TOV) %&gt;% as.data.frame()

slr_vif &lt;-  lm(PTS ~ ., data = ds_vif)

car::vif(slr_vif)</code></pre>
<pre><code>##     FG2A     FG3A      FTA      AST     OREB       PF      TOV 
## 1.746619 1.195806 1.407422 1.408611 1.284540 1.134261 1.372683</code></pre>
<p>Since all of those predictors are close to one, they can stay in the model for now. If VIF gets closer to 5, then we can start worrying about multicollinearity.</p>
<p><br></p>
</div>
<div id="outliers" class="section level3">
<h3>Outliers</h3>
<pre class="r"><code>slr_show_any_outlier_plz &lt;- lm(PTS ~ FTA, data = nba)
plot(slr_show_any_outlier_plz$residuals)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Built one poor model for purpose to show you that residuals plot can also be used to find outliers in the data.</p>
<p>Outliers are observations which <strong>response variable values</strong> are far away from the mean (there are different methods for tagging outliers - 3 standard deviations, interquantile range, median absolute deviations etc.). Some machine learning techniques are quite outlier-proof, but linear regression is not one of them. If you keep outlying observations in dataset, model still will try to fit them into general regression line creating skewed line.</p>
<p>It’s good to inspect why we got so extreme values in a dataset. Of course sometimes it is proper observation, but other times it can be caused by human or system error and therefore it should be cut from dataset or updated. Common technique is to impute mean/median value instead of outlier.</p>
<p>If we feel we have enough observations it might be easier to just remove it.</p>
<p>Initially to find outliers we can use density plots and/or boxplots to highlight them.</p>
<pre class="r"><code>boxplot(nba$PTS)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>plot(density(nba$PTS))</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Similar to outliers are…</p>
<p><br></p>
</div>
<div id="high-leverage-observations" class="section level3">
<h3>High leverage observations</h3>
<p>Which are basically outliers, but on predictors variable. Training dataset having outliers on independent variables can have strongly affected fit line and become biased.</p>
<p>Observations with high leverage predictors and outlying response are known as influential observations and they have huge power to affect final slope of the regression line.</p>
<p>To calculate the impact of each data point in linear regression, we can use Cook’s distance measure. D statistic is defined as sum of all the changes in the regression model then given observation is removed from the training data.</p>
<pre class="r"><code>cooked &lt;- cooks.distance(slr_vif)

plot(cooked)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Just out of curiosity.. what were those games:</p>
<pre class="r"><code>index &lt;- names(cooked[cooked &gt; 0.006])

nba[index,] %&gt;%
  select(PLAYER_NAME,GAME_DATE, PTS, FG2A, FG3A, FTA, OREB)</code></pre>
<pre><code>## # A tibble: 2 x 7
##   PLAYER_NAME    GAME_DATE    PTS  FG2A  FG3A   FTA  OREB
##   &lt;chr&gt;          &lt;date&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1 Andre Drummond 2016-01-20    17     4     0    36     5
## 2 DeAndre Jordan 2015-11-30    18     6     0    34     9</code></pre>
<p>Ahh, the hack-a-center games… makes total sense. 36 and 34 free throws.</p>
</div>
<div id="scaling-variables" class="section level3">
<h3>Scaling variables</h3>
<p>Finally, we may want to scale variables to common range, using standardization ( for example to 0:1 or -1:1 range) or normalization (Z-score) to obtain variables from the same range of values. That ensures model will not be skewed by one variable having totally different scale from the rest.</p>
</div>
</div>
<div id="building-a-model---example-linear-regression-with-mlr-package" class="section level2">
<h2>Building a model - example Linear Regression with mlr package</h2>
<p>The theory part is finally over!</p>
<p>Did you just scrolled down here to get your part of code and totally skipped previous chunk? :(</p>
<p>Anyway, let’s try to go through all the checks and assumptions in practice, using mlr package:</p>
<div id="correlation-based-feature-selection" class="section level3">
<h3>Correlation based feature selection</h3>
<pre class="r"><code>## preparing data
ds &lt;- nba %&gt;%
  select(PTS,FG2A, FG3A, FTA, AST, OREB, PF, TOV, STL, PACE) %&gt;% as.data.frame()

## correlation matrix
cm &lt;- cor(ds)
corrplot::corrplot(cm, method = &#39;number&#39;, diag = F, type = &#39;upper&#39;) </code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Correlation matrix shows that variable Pace is uncorrelated with PTS at all. At least 5 variables seem to well correlated to PTS, there is no reason to be overly worried by multicollinearity. For beginning I will pick all variables except Pace and see what happens.</p>
<pre class="r"><code>ds &lt;- ds %&gt;%
  select( - PACE)</code></pre>
</div>
<div id="first-basic-model" class="section level3">
<h3>First basic model</h3>
<pre class="r"><code>## defining learning task
task &lt;- makeRegrTask(id = &#39;nba&#39;, data  = ds, target = &#39;PTS&#39;)

## defining learner: simple linear regression
learner &lt;- makeLearner(cl = &#39;regr.lm&#39;)

## This time keeping things simple, going with simple holdout validation
ho &lt;- makeResampleInstance(&quot;Holdout&quot;,task)
task_train &lt;- subsetTask(task, ho$train.inds[[1]])
task_test &lt;- subsetTask(task, ho$test.inds[[1]])

## Training basic model with all variables, without any transformations and tests:
model_trained_0 &lt;- train(learner, task_train)
newdata_pred_0 &lt;- predict(model_trained_0, task_test)

## Check RMSE, MSE and R^2
performance(newdata_pred_0,  measures = list(mse,rmse,rsq))</code></pre>
<pre><code>##       mse      rmse       rsq 
## 11.003556  3.317161  0.831295</code></pre>
<p>Let’s check using cross validation</p>
<pre class="r"><code>rdesc &lt;- makeResampleDesc(method = &quot;RepCV&quot;, reps = 2, folds = 5)
## instance for the splitting to check all models on the same conditions
rin = makeResampleInstance(rdesc, task = task)

## fourth, resample:

r &lt;- resample(
              learner = learner,
              task = task,
              resampling = rdesc, 
              measures = list(mse,rmse,rsq),
              models = T, show.info = FALSE
              )
print(r)</code></pre>
<pre><code>## Resample Result
## Task: nba
## Learner: regr.lm
## Aggr perf: mse.test.mean=10.9240522,rmse.test.rmse=3.3051554,rsq.test.mean=0.8298947
## Runtime: 1.2772</code></pre>
<p>R-squared is quite high, RMSE on training dataset looks decent… lets check the real vs predicted plot:</p>
<pre class="r"><code>plotResiduals(newdata_pred_0)</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39;</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<pre class="r"><code>broom::tidy(model_trained_0$learner.model)</code></pre>
<pre><code>##          term    estimate   std.error  statistic
## 1 (Intercept) -0.47108412 0.029499382 -15.969287
## 2        FG2A  1.01131207 0.004328380 233.646765
## 3        FG3A  1.19560942 0.006020440 198.591693
## 4         FTA  0.80804045 0.006176252 130.830223
## 5         AST -0.04341453 0.007031087  -6.174655
## 6        OREB -0.05337875 0.012183867  -4.381101
## 7          PF  0.06146833 0.010627657   5.783808
## 8         TOV  0.04661127 0.012127939   3.843297
## 9         STL  0.02186359 0.015847430   1.379630
##                                                             p.value
## 1 0.000000000000000000000000000000000000000000000000000000002855204
## 2 0.000000000000000000000000000000000000000000000000000000000000000
## 3 0.000000000000000000000000000000000000000000000000000000000000000
## 4 0.000000000000000000000000000000000000000000000000000000000000000
## 5 0.000000000667927418573255173364633385801880649523809552192687988
## 6 0.000011830962164933312005463128535609484970336779952049255371094
## 7 0.000000007344145980840371659616877542120505495404358953237533569
## 8 0.000121535563116167445989884710044748317159246653318405151367187
## 9 0.167706448485313130980500773148378357291221618652343750000000000</code></pre>
<p>First thought: there are outliers that should be treated, but first lets find out what the residuals plots present:</p>
<pre class="r"><code>plot(model_trained_0$learner.model, which = 1)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Residuals are close to show normal distribution for values between 10 and 30, but outside of that range it is not so nice.</p>
<pre class="r"><code>plot(model_trained_0$learner.model, which = 2)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Distribution of residuals is definately not normal.</p>
<pre class="r"><code>plot(model_trained_0$learner.model, which = 3)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>There is heteroscedaskicity as well</p>
<pre class="r"><code>plot(model_trained_0$learner.model, which = 4)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Woooh…</p>
<p>Let’s also run statistical test for each effect, just to have confirmation.</p>
<pre class="r"><code>lmtest::bptest(model_trained_0$learner.model)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  model_trained_0$learner.model
## BP = 7635, df = 8, p-value &lt; 0.00000000000000022</code></pre>
<pre class="r"><code>lmtest::dwtest(model_trained_0$learner.model)</code></pre>
<pre><code>## 
##  Durbin-Watson test
## 
## data:  model_trained_0$learner.model
## DW = 2.0086, p-value = 0.8369
## alternative hypothesis: true autocorrelation is greater than 0</code></pre>
<pre class="r"><code>shapiro.test(model_trained_0$learner.model$residuals[1:5000])</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  model_trained_0$learner.model$residuals[1:5000]
## W = 0.98784, p-value &lt; 0.00000000000000022</code></pre>
<p>So:</p>
<p>Heterescedaskicity: check
Non-normal distribution: check
Outliers/Influential observations: check
Autocorrelation: no..yay!</p>
<p>I am going to start with removing influential observations and outliers - there is a chance it will reduce other negative effects.</p>
</div>
<div id="model-with-excludedscaled-observations" class="section level3">
<h3>Model with excluded/scaled observations</h3>
<p>I want to do 4 steps:</p>
<ul>
<li>Remove influential observations measured by cook’s distance</li>
<li>Remove outliers in Points and Free throws</li>
<li>Use only FTA, FG2A and FG3A as they have decent correlation with PTS</li>
<li>Log scale predictors, since they distributions are skewed to the right</li>
</ul>
<pre class="r"><code>index2 &lt;- as.numeric(names(cooked[cooked &gt; 0.001])) ## removing by cook distance
index3 &lt;- which(ds$PTS &gt; 50)                   ## removing outliers
index4 &lt;- which(ds$FTA &gt; 15) ## removing outliers

ds_tmp0 &lt;- ds[ - c(index2, index3, index4),c(&#39;PTS&#39;,&#39;FTA&#39;,&#39;FG2A&#39;,&#39;FG3A&#39;)]</code></pre>
<pre class="r"><code>ds_tmp0 &lt;- ds_tmp0 %&gt;% 
  mutate(FG3A = log(FG3A),
         FTA = log(FTA),
         FG2A = log(FG2A)
         )
         
is.na(ds_tmp0) &lt;- sapply(ds_tmp0, is.infinite)
ds_new &lt;- ds_tmp0 %&gt;% tidyimpute::impute_zero()


task &lt;- makeRegrTask(id = &#39;nba&#39;, data  = ds_new, target = &#39;PTS&#39;)

learner &lt;- makeLearner(cl = &#39;regr.lm&#39;)

## This time keeping things simple, going with simple holdout validation
ho &lt;- makeResampleInstance(&quot;Holdout&quot;,task)
task_train &lt;- subsetTask(task, ho$train.inds[[1]])
task_test &lt;- subsetTask(task, ho$test.inds[[1]])

## Training basic model with all variables, without any transformations and tests:
model_trained_0 &lt;- train(learner, task_train)
newdata_pred_0 &lt;- predict(model_trained_0, task_test)

## Check RMSE, MSE and R^2
performance(newdata_pred_0,  measures = list(mse,rmse,rsq))</code></pre>
<pre><code>##       mse      rmse       rsq 
## 15.675226  3.959195  0.748217</code></pre>
<pre class="r"><code>plotResiduals(newdata_pred_0)</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39;</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p>Let’s check using cross validation again</p>
<pre class="r"><code>rdesc &lt;- makeResampleDesc(method = &quot;RepCV&quot;, reps = 2, folds = 5)
## instance for the splitting to check all models on the same conditions
rin = makeResampleInstance(rdesc, task = task)

## fourth, resample:

r &lt;- resample(
              learner = learner,
              task = task,
              resampling = rdesc, 
              measures = list(mse,rmse,rsq),
              models = T, show.info = FALSE
              )
print(r)</code></pre>
<pre><code>## Resample Result
## Task: nba
## Learner: regr.lm
## Aggr perf: mse.test.mean=15.3804608,rmse.test.rmse=3.9217931,rsq.test.mean=0.7531529
## Runtime: 0.542082</code></pre>
<pre class="r"><code>plot(model_trained_0$learner.model, which = 1)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<pre class="r"><code>plot(model_trained_0$learner.model, which = 2)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<pre class="r"><code>plot(model_trained_0$learner.model, which = 3)</code></pre>
<p><img src="/blog/mlr-linear-regression_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>Transformations helped lower heteroscedaskicity and non normal distribution for observations which has value of Points no larger than 30-35. There is still problem affecting high-scoring performances.</p>
<p>Even though model performance metrics are worse for the second model, I think that tweaked version has better quality of prediction, especially for lower values of points. What would help solve the issue, is another variable describing if given player is an all-star (those players are more likely to score over 30 points).</p>
<p>Another possibility is to use split the model into 2: one working for lower PTS values, one for larger.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Although model wasn’t perfect I am quite happy with the work described in the article. We went through absoulte basics of OLS and gradient descent, through correlation, linear regression idea, assumptions, test, mlr code and possible fixes. It was first larger blogpost from the series, therefore a bit chaotic. Hope the next one would be better. Feel free to message me via email or twitter and say what you think!</p>
</div>


        </div>

      </div>

      <div id="footer">
        Copyright 2019 
      </div>

    </div>
    
    <script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  </body>
</html>


